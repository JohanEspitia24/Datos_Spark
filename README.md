# Taller de SQL con **PySpark**Â ğŸ“ŠğŸ”¥  
Procesamiento de Alto Volumen de Datos

> **Notebook principal:** `Espitia_Cobos_SQL_Spark.ipynb`  
> **Autor:** **JohanÂ AlejandroÂ EspitiaÂ Cobos** â€“ PontificiaÂ UniversidadÂ Javeriana

---

## ğŸ“Œ DescripciÃ³n

Este proyecto muestra, pasoâ€¯aâ€¯paso, cÃ³mo utilizar **ApacheÂ Spark** (vÃ­a PySpark) para ejecutar consultas SQL sobre datos estructurados de forma eficiente y escalable.  
Dentro del cuaderno se construyen pequeÃ±os _datasets_, se transforman en **DataFrames**, se crean vistas temporales y tablas administradas (Hive), y se ilustran operaciones fundamentales de **SparkÂ SQL**:

1. Inicializar la **SparkSession**.  
2. Crear y transformar **DataFrames**.  
3. Clasificar registros con columnas calculadas (`when`, `otherwise`).  
4. Registrar **vistas temporales** y disparar consultas con `spark.sql()`.  
5. Persistir resultados como **tablas** con `write.saveAsTable()`.  
6. Insertar, consultar y filtrar datos directamente desde SQL.  
7. _(SecciÃ³n futura)_ IntroducciÃ³n a **RDDâ€¯API**.  
8. Conclusiones y reflexiones sobre el desempeÃ±o de Spark.

El notebook sirve como material didÃ¡ctico para la asignatura **Procesamiento de Datos a Gran Escala** y como punto de partida para desarrollos mÃ¡s avanzados sobre Spark.

---

## ğŸ—ï¸ Estructura del repositorio


---

## âš™ï¸ Requisitos

| Componente    | VersiÃ³n recomendada |
|---------------|---------------------|
| Python        | 3.9Â o superior      |
| ApacheÂ Spark  | 3.4.xÂ /Â 3.5.x       |
| PySpark       | igual a la versiÃ³n de Spark |
| JupyterLab    | 4.x                 |

> ğŸ’¡ **Alternativa rÃ¡pida:** usar la imagen Docker oficial `jupyter/pyspark-notebook`.

---

## ğŸš€ Puesta en marcha local

1. **Clonar el repositorio**

   ```bash
   git clone https://github.com/<TU_USUARIO>/<TU_REPOSITORIO>.git
   cd <TU_REPOSITORIO>
